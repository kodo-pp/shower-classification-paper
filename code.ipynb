{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c059fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import tqdm.notebook\n",
    "import sklearn.metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple, List, Optional, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f321198",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'eplus': h5py.File('data/eplus_angle_position_5deg_xy.h5'),\n",
    "    'piplus': h5py.File('data/piplus_angle_position_5deg_xy.h5'),\n",
    "    'gamma': h5py.File('data/gamma_angle_position_5deg_xy.h5'),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb84831e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print('Models will be run on a GPU (CUDA/HIP)')\n",
    "    dev = torch.device('cuda')\n",
    "else:\n",
    "    print('Models will be run on a CPU')\n",
    "    dev = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e2934f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_sizes(size_a: int, size_b: int, num_samples: int) -> Tuple[int, int]:\n",
    "    num_a_samples = min(\n",
    "        np.random.binomial(num_samples, size_a / (size_a + size_b)),\n",
    "        size_a,\n",
    "    )\n",
    "    num_b_samples = min(num_samples - num_a_samples, size_b)\n",
    "    num_a_samples = min(num_samples - num_b_samples, size_a)\n",
    "    assert num_a_samples + num_b_samples == min(num_samples, size_a + size_b)\n",
    "    return num_a_samples, num_b_samples\n",
    "\n",
    "class SamplerEpoch:\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_a_indices: int,\n",
    "        num_b_indices: int,\n",
    "        minibatch_size: int,\n",
    "        id: Optional[int],\n",
    "        cursor_a: int = 0,\n",
    "        cursor_b: int = 0\n",
    "    ) -> None:\n",
    "        self.minibatch_size = minibatch_size\n",
    "        self.id = id\n",
    "        self.num_a_indices = num_a_indices\n",
    "        self.num_b_indices = num_b_indices\n",
    "        self.initial_cursor_a = cursor_a\n",
    "        self.initial_cursor_b = cursor_b\n",
    "        self.cursor_a = cursor_a\n",
    "        self.cursor_b = cursor_b\n",
    "    \n",
    "    def __iter__(self) -> 'SamplerEpoch':\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        num_a_items_left = self.num_a_indices - self.cursor_a\n",
    "        num_b_items_left = self.num_b_indices - self.cursor_b\n",
    "        assert num_a_items_left >= 0\n",
    "        assert num_b_items_left >= 0\n",
    "        \n",
    "        if num_a_items_left == num_b_items_left == 0:\n",
    "            raise StopIteration()\n",
    "\n",
    "        num_a_items, num_b_items = sample_sizes(\n",
    "            num_a_items_left,\n",
    "            num_b_items_left,\n",
    "            self.minibatch_size,\n",
    "        )\n",
    "        \n",
    "        begin_a = self.cursor_a\n",
    "        end_a = begin_a + num_a_items\n",
    "        begin_b = self.cursor_b\n",
    "        end_b = begin_b + num_b_items\n",
    "        \n",
    "        self.cursor_a = end_a\n",
    "        self.cursor_b = end_b\n",
    "        # Only the last mini-batch may be incomplete.\n",
    "        assert num_a_items + num_b_items == self.minibatch_size or (\n",
    "            self.cursor_a == self.num_a_indices\n",
    "            and self.cursor_b == self.num_b_indices\n",
    "        )\n",
    "        return (begin_a, end_a), (begin_b, end_b)\n",
    "    \n",
    "    def __len__(self):\n",
    "        num_indices = self.num_a_indices + self.num_b_indices - self.initial_cursor_a - self.initial_cursor_b\n",
    "        batch = self.minibatch_size\n",
    "        return (num_indices + batch - 1) // batch\n",
    "    \n",
    "    def __repr__(self) -> str:\n",
    "        return (\n",
    "            f'SamplerEpoch(id={self.id}), '\n",
    "            f'minibatch_size={self.minibatch_size}, '\n",
    "            f'num_indices=({self.num_a_indices}, {self.num_b_indices}), '\n",
    "            f'cursor=({self.cursor_a}, {self.cursor_b}))'\n",
    "        )\n",
    "\n",
    "class Sampler:\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_samples_a: int,\n",
    "        num_samples_b: int,\n",
    "        minibatch_size: int,\n",
    "        cv_split: float,\n",
    "    ) -> None:\n",
    "        while True:\n",
    "            num_cv_indices_a = round(num_samples_a * cv_split)\n",
    "            num_cv_indices_b = round(num_samples_b * cv_split)\n",
    "            self.cv_start_a = num_samples_a - num_cv_indices_a\n",
    "            self.cv_start_b = num_samples_b - num_cv_indices_b\n",
    "            if (self.cv_start_a + self.cv_start_b) % minibatch_size == 1:\n",
    "                cv_split *= 0.9999\n",
    "            else:\n",
    "                break\n",
    "        self.num_samples_a = num_samples_a\n",
    "        self.num_samples_b = num_samples_b\n",
    "        self.minibatch_size = minibatch_size\n",
    "        self.num_epochs = 0\n",
    "    \n",
    "    def epoch(self) -> SamplerEpoch:\n",
    "        self.num_epochs += 1\n",
    "        return SamplerEpoch(\n",
    "            self.cv_start_a,\n",
    "            self.cv_start_a,\n",
    "            minibatch_size=self.minibatch_size,\n",
    "            id=self.num_epochs,\n",
    "        )\n",
    "    \n",
    "    def cv(self) -> SamplerEpoch:\n",
    "        return SamplerEpoch(\n",
    "            self.num_samples_a,\n",
    "            self.num_samples_b,\n",
    "            minibatch_size=self.minibatch_size,\n",
    "            id=None,\n",
    "            cursor_a=self.cv_start_a,\n",
    "            cursor_b=self.cv_start_b,\n",
    "        )\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return (\n",
    "            f'Sampler(cv_start=({self.cv_start_a}, {self.cv_start_b}), '\n",
    "            f'num_samples=({self.num_samples_a}, {self.num_samples_b}), '\n",
    "            f'minibatch_size={self.minibatch_size}, '\n",
    "            f'self.num_epochs={self.num_epochs})'\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794b443a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "def get_layers(data_file: h5py.File, begin: int, end: int) -> List[np.ndarray]:\n",
    "    layers = []\n",
    "    for i in range(3):\n",
    "        layer = data_file[f'layer_{i}']\n",
    "        layer_part = layer[begin:end]\n",
    "        layers.append(layer_part)\n",
    "    return layers\n",
    "\n",
    "def flatten_layers(data_file: h5py.File, begin: int, end: int) -> np.ndarray:\n",
    "    layers = get_layers(data_file, begin, end)\n",
    "    \n",
    "    for i, layer in enumerate(layers):\n",
    "        feature_size = reduce(lambda a, b: a * b, layer.shape[1:])\n",
    "        reshaped = layer.reshape((layer.shape[0], feature_size))\n",
    "        layers[i] = reshaped\n",
    "        \n",
    "    return np.hstack(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7feb0aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import abstractmethod, ABC\n",
    "\n",
    "class BaseModel(torch.nn.Module, ABC):\n",
    "    @abstractmethod\n",
    "    def prepare_input(\n",
    "        self,\n",
    "        data_a: h5py.File,\n",
    "        data_b: h5py.File,\n",
    "        begin_a: int,\n",
    "        end_a: int,\n",
    "        begin_b: int,\n",
    "        end_b: int,\n",
    "    ) -> Tuple[Any, torch.Tensor]:\n",
    "        \"\"\"Prepare input data from the dataset slices and return tensors `X` and `y` on `dev`.\"\"\"\n",
    "        ...\n",
    "    \n",
    "    @abstractmethod\n",
    "    def description(self) -> str:\n",
    "        \"\"\"Briefly describe this model.\"\"\"\n",
    "        ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7419fbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelFcPixel(BaseModel):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = torch.nn.Sequential(\n",
    "            self.make_layer(504, 512),\n",
    "            self.make_layer(512, 1024),\n",
    "            self.make_layer(1024, 2048),\n",
    "            self.make_layer(2048, 1024),\n",
    "            self.make_layer(1024, 128),\n",
    "            torch.nn.Linear(128, 1),\n",
    "            torch.nn.Sigmoid(),\n",
    "        )\n",
    "    \n",
    "    @staticmethod\n",
    "    def make_layer(num_in, num_out):\n",
    "        return torch.nn.Sequential(\n",
    "            torch.nn.Linear(num_in, num_out),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.Dropout(),\n",
    "            torch.nn.BatchNorm1d(num_out),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def prepare_input(\n",
    "        data_a: h5py.File,\n",
    "        data_b: h5py.File,\n",
    "        begin_a: int,\n",
    "        end_a: int,\n",
    "        begin_b: int,\n",
    "        end_b: int,\n",
    "    ) -> Tuple[Any, torch.Tensor]:\n",
    "        x_a = flatten_layers(data_a, begin_a, end_a)\n",
    "        x_b = flatten_layers(data_b, begin_b, end_b)\n",
    "        x = torch.Tensor(np.vstack([x_a, x_b])).to(dev)\n",
    "        y = torch.Tensor([0] * (end_a - begin_a) + [1] * (end_b - begin_b)).to(dev)\n",
    "        return x, y\n",
    "    \n",
    "    @staticmethod\n",
    "    def description() -> str:\n",
    "        return 'FC network on pixel intensities'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e55ecb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AssertShape(torch.nn.Module):\n",
    "    def __init__(self, *shape):\n",
    "        super().__init__()\n",
    "        self.required_shape = torch.Size(shape)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        actual_shape = x.shape[1:]\n",
    "        if actual_shape != self.required_shape:\n",
    "            raise ValueError(f'Invalid shape: expected {self.required_shape}, got {actual_shape}')\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3a8bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in data['eplus'].items():\n",
    "    print(f'{key} => {value.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e135711a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelConv3(BaseModel):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers_0 = torch.nn.Sequential(\n",
    "            AssertShape(1, 3, 96),\n",
    "            self.make_conv_layer(1, 16),\n",
    "            self.make_conv_layer(16, 32),\n",
    "            torch.nn.MaxPool2d((1, 2)),\n",
    "            AssertShape(32, 3, 48),\n",
    "            self.make_conv_layer(32, 64),\n",
    "            self.make_conv_layer(64, 96),\n",
    "            torch.nn.MaxPool2d((1, 2)),\n",
    "            AssertShape(96, 3, 24),\n",
    "            self.make_conv_layer(96, 128),\n",
    "            self.make_conv_layer(128, 256),\n",
    "            torch.nn.MaxPool2d((1, 2)),\n",
    "            AssertShape(256, 3, 12),\n",
    "            self.make_conv_layer(256, 32, kernel=1),\n",
    "            AssertShape(32, 3, 12),\n",
    "            torch.nn.Flatten(),\n",
    "            AssertShape(1152),\n",
    "        )\n",
    "        self.layers_1 = torch.nn.Sequential(\n",
    "            AssertShape(1, 12, 12),\n",
    "            self.make_conv_layer(1, 16),\n",
    "            self.make_conv_layer(16, 32),\n",
    "            self.make_conv_layer(32, 64),\n",
    "            torch.nn.MaxPool2d(2),\n",
    "            AssertShape(64, 6, 6),\n",
    "            self.make_conv_layer(64, 96),\n",
    "            self.make_conv_layer(96, 128),\n",
    "            self.make_conv_layer(128, 256),\n",
    "            torch.nn.MaxPool2d(2),\n",
    "            AssertShape(256, 3, 3),\n",
    "            self.make_conv_layer(256, 64, kernel=1),\n",
    "            AssertShape(64, 3, 3),\n",
    "            torch.nn.Flatten(),\n",
    "            AssertShape(576),\n",
    "        )\n",
    "        self.layers_2 = torch.nn.Sequential(\n",
    "            AssertShape(1, 12, 6),\n",
    "            self.make_conv_layer(1, 16),\n",
    "            self.make_conv_layer(16, 32),\n",
    "            self.make_conv_layer(32, 64),\n",
    "            torch.nn.MaxPool2d((2, 1)),\n",
    "            AssertShape(64, 6, 6),\n",
    "            self.make_conv_layer(64, 96),\n",
    "            self.make_conv_layer(96, 128),\n",
    "            self.make_conv_layer(128, 256),\n",
    "            torch.nn.MaxPool2d(2),\n",
    "            AssertShape(256, 3, 3),\n",
    "            self.make_conv_layer(256, 48, kernel=1),\n",
    "            AssertShape(48, 3, 3),\n",
    "            torch.nn.Flatten(),\n",
    "            AssertShape(432),\n",
    "        )\n",
    "        flat_size = 1152 + 576 + 432\n",
    "        self.layers_post = torch.nn.Sequential(\n",
    "            AssertShape(flat_size),\n",
    "            self.make_fc_layer(flat_size, 512),\n",
    "            self.make_fc_layer(512, 128),\n",
    "            self.make_fc_layer(128, 1),\n",
    "            torch.nn.Sigmoid(),\n",
    "        )\n",
    "    \n",
    "    @staticmethod\n",
    "    def make_conv_layer(num_in, num_out, kernel=3):\n",
    "        return torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(num_in, num_out, kernel, padding='same'),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.Dropout(),\n",
    "            torch.nn.BatchNorm2d(num_out),\n",
    "        )\n",
    "    \n",
    "    @staticmethod\n",
    "    def make_fc_layer(num_in, num_out):\n",
    "        return torch.nn.Sequential(\n",
    "            torch.nn.Linear(num_in, num_out),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.Dropout(),\n",
    "            torch.nn.BatchNorm1d(num_out),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x0, x1, x2 = x\n",
    "        p0 = self.layers_0(x0)\n",
    "        p1 = self.layers_1(x1)\n",
    "        p2 = self.layers_2(x2)\n",
    "        p = torch.cat((p0, p1, p2), 1)\n",
    "        return self.layers_post(p)\n",
    "\n",
    "    @staticmethod\n",
    "    def prepare_input(\n",
    "        data_a: h5py.File,\n",
    "        data_b: h5py.File,\n",
    "        begin_a: int,\n",
    "        end_a: int,\n",
    "        begin_b: int,\n",
    "        end_b: int,\n",
    "    ) -> Tuple[Any, torch.Tensor]:\n",
    "        x0_a, x1_a, x2_a = get_layers(data_a, begin_a, end_a)\n",
    "        x0_b, x1_b, x2_b = get_layers(data_b, begin_b, end_b)\n",
    "        x0 = torch.Tensor(np.vstack([x0_a, x0_b])).to(dev)\n",
    "        x1 = torch.Tensor(np.vstack([x1_a, x1_b])).to(dev)\n",
    "        x2 = torch.Tensor(np.vstack([x2_a, x2_b])).to(dev)\n",
    "        \n",
    "        assert len(x0.shape) == len(x1.shape) == len(x2.shape) == 3\n",
    "        x0 = x0.view(x0.shape[0], 1, x0.shape[1], x0.shape[2])\n",
    "        x1 = x1.view(x1.shape[0], 1, x1.shape[1], x1.shape[2])\n",
    "        x2 = x2.view(x2.shape[0], 1, x2.shape[1], x2.shape[2])\n",
    "        assert len(x0.shape) == len(x1.shape) == len(x2.shape) == 4\n",
    "        \n",
    "        x = (x0, x1, x2)\n",
    "        y = torch.Tensor([0] * (end_a - begin_a) + [1] * (end_b - begin_b)).to(dev)\n",
    "        return x, y\n",
    "    \n",
    "    @staticmethod\n",
    "    def description() -> str:\n",
    "        return '3-stream convolution network'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba30ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(\n",
    "    model,\n",
    "    alt_label,\n",
    "    num_epochs,\n",
    "    minibatch_size=128,\n",
    "    cv_split=0.1,\n",
    "):\n",
    "    data_a = data['eplus']\n",
    "    data_b = data[alt_label]\n",
    "    sampler = Sampler(\n",
    "        len(data_a['layer_0']),\n",
    "        len(data_b['layer_0']),\n",
    "        minibatch_size=minibatch_size,\n",
    "        cv_split=cv_split,\n",
    "    )\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    max_train_index_a = 0\n",
    "    max_train_index_b = 0\n",
    "    model.train()\n",
    "    while True:\n",
    "        epoch = sampler.epoch()\n",
    "        if epoch.id > num_epochs:\n",
    "            break\n",
    "\n",
    "        print(f':: Epoch {epoch.id}', flush=True)\n",
    "        for (begin_a, end_a), (begin_b, end_b) in tqdm.notebook.tqdm(epoch):\n",
    "            assert begin_a <= end_a\n",
    "            assert begin_b <= end_b\n",
    "            \n",
    "            max_train_index_a = max([max_train_index_a, end_a-1])\n",
    "            max_train_index_b = max([max_train_index_b, end_b-1])\n",
    "            opt.zero_grad()\n",
    "            x, y = model.prepare_input(data_a, data_b, begin_a, end_a, begin_b, end_b)\n",
    "            y_prob = model(x).flatten()\n",
    "            loss = F.binary_cross_entropy(y_prob, y)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "    \n",
    "    print(':: Evaluating')\n",
    "    model.eval()\n",
    "    y = []\n",
    "    y_prob = []\n",
    "    with torch.no_grad():\n",
    "        for (begin_a, end_a), (begin_b, end_b) in tqdm.notebook.tqdm(sampler.cv()):\n",
    "            assert max_train_index_a < begin_a <= end_a\n",
    "            assert max_train_index_b < begin_b <= end_b\n",
    "            x, this_y = model.prepare_input(data_a, data_b, begin_a, end_a, begin_b, end_b)\n",
    "            this_y_prob = model(x).flatten().cpu()\n",
    "            y += list(this_y.cpu())\n",
    "            y_prob += list(this_y_prob)\n",
    "    \n",
    "    return model, y, y_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7816f5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model: torch.nn.Module):\n",
    "    # https://stackoverflow.com/a/49201237\n",
    "    return sum(param_set.numel() for param_set in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f44709c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_big_number(num: int) -> str:\n",
    "    it = iter(reversed(str(num)))\n",
    "    result = []\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            for _ in range(3):\n",
    "                result.append(next(it))\n",
    "            result.append(' ')\n",
    "        except StopIteration:\n",
    "            break\n",
    "        \n",
    "    if result[-1] == ' ':\n",
    "        result.pop()\n",
    "    return f''.join(reversed(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21735c2a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "models = [\n",
    "    ModelConv3().to(dev),\n",
    "    ModelFcPixel().to(dev),\n",
    "]\n",
    "num_epochs = 2\n",
    "alt_label = 'piplus'\n",
    "\n",
    "results = {}\n",
    "for model in models:\n",
    "    print(f'Model: {model.description()}')\n",
    "    print(f'Parameters: {format_big_number(count_parameters(model))}')\n",
    "    _, y, y_prob = train_and_evaluate(\n",
    "        model=model,\n",
    "        alt_label=alt_label,\n",
    "        num_epochs=num_epochs,\n",
    "    )\n",
    "    results[model.__class__.__name__] = (y, y_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bc26fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fpr_threshold = 1/1000\n",
    "tpr_threshold = 0.9\n",
    "\n",
    "labels = []\n",
    "handles = []\n",
    "\n",
    "for name, (y, y_prob) in results.items():\n",
    "    fpr, tpr, _ = sklearn.metrics.roc_curve(y, y_prob)\n",
    "    mask = (fpr > fpr_threshold) & (tpr > tpr_threshold)\n",
    "    handles += plt.plot(tpr[mask], 1/fpr[mask])\n",
    "    labels.append(name)\n",
    "\n",
    "plt.xlabel('True positive (π⁺) rate')\n",
    "plt.ylabel('1 / False positive (π⁺) rate')\n",
    "plt.legend(handles=handles, labels=labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a30884",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, (y, y_prob) in results.items():\n",
    "    auc = sklearn.metrics.roc_auc_score(y, y_prob)\n",
    "    print(f'{name:20} ROC AUC = {auc:.7f}; 1/(1-ROC AUC) = {1/(1-auc):.7f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87011825",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
